---
layout: post
title:  "风控建模知识点梳理-技术篇"
date:   2022-02-21
categories: 风控建模
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

## ***前言***
自从进入风控领域以后，日常总会遇到稀奇古怪的问题，归根到底还是基础不牢固，因此应该时刻对平时遇到的问题作出总结，记录在博客中，方便自己回顾，同时保持学习的热情。本篇主要从**技术角度**分析信贷风控模型所需要注意的知识点。

## **数据分箱**
### **离散的优势**
(1) 离散后的特征对异常数据具有较强的鲁棒性。例如当年龄>50是1，否则为0，如果不进行上述离散化处理，一旦出现年龄为300则会对模型产生很大干扰。

(2) 逻辑回归属于广义线性模型，表达能力有限，将单变量离散化后可以给每个分段赋予单独的权重，相当于为模型引入了非线性，提升模型表达能力。

(3) 离散后可进行特征交叉，进一步增强非线性能力。举一个最简单的例子，两个特征：年龄和性别，可以组合成 年龄_性别 的一个新特征，比如$M_18$，$F_22$等等，然后再对这个特征做one hot编码，即可得到新的特征属性值（暴力交叉可能产生的稀疏性问题参考*FM*和*FFM*解决方案）

(4) 可以把缺失值作为一类带入模型考虑

(5) 可以所有变量运用到相似尺度上

(6) 运算速度更快

### **分箱方法**
有监督分箱：<font color=#00ffff>  卡方分箱 </font>，Best-KS分箱

无监督分享：<font color=#00ffff>  等频分箱 </font>，<font color=#00ffff>  等距分箱 </font>，聚类分箱

（后期补充具体方法）

### **WOE，IV**
(1) 计算方法

$$WOE_i = \ln(\frac{py_i}{pn_i}) = ln(\frac{\frac{y_i}{y_T}}{\frac{n_i}{n_T}})$$

分别代表响应客户占比和未响应客户占比的比值

$$IV_i = (py_i-pn_i)*WOE_i$$

## **数据质量**
### **缺失率**
*待补充*
### **集中度**
*待补充*
## **算法对比**
### **XGBOOST，GBDT，RF，LightGBM**

(算法推导部分于单独章节补充，本章参考["这里"](https://blog.csdn.net/yimingsilence/article/details/82193890))

(1) 一切基础，决策树(太基础的就不谈了，一棵树而已)

- ID3决策树中我们使用**信息增益**来指导分裂，其概念为得知a属性后使得样本集合不确定性减少的程度，即信息熵-条件熵。找到最大信息熵的那个特征进行分裂，然后对于每个分裂的子节点循环使用信息增益分裂。具体参考["这位朋友的分享"](https://zhuanlan.zhihu.com/p/26760551)。但是ID3算法拥有致命的问题，即该算法对可取值数目最多的属性有所偏好，因此引出C4.5算法
- C4.5决策树中我们使用**信息增益率**来指导分裂，改善了ID3的不足
- CART回归树，是一个二叉树，通过遍历变量和切分点，找到可以使平方损失函数最小的【变量，切分点】，划分区域后计算每个区域的平均值作为输出值，并且循环使用上述方法直到达到要求。分类树使用基尼系数

(2) RF,GBDT属于集成学习，继承多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。而XGBOOST和lightGBM是GBDT优秀的工程实现和改进。

> 随机森林，Bagging算法延伸

- 步骤为 (1)随机选择样本(**有放回**) (2)随机选择特征(**延伸点**) (3)构建决策树 (4) 随机森林投票
- 特征为 (1)不剪枝，回归任务平均法，分类任务投票法 (2)不用对其交叉验证，生长过程中对误差进行无偏估计（每个学习器只是用了训练集中63.8%的样本）

> GBDT，Boosting算法延伸

- 传统Adaboost更新分类器权重和样本权重，如果样本被错误预测则加大权重，GBDT是不断拟合树的残差(**延伸点**)
- GBDT的树都是CART回归树，不论是分类还是回归任务。

> XGBOOST, GBDT延伸

- GBDT优化时仅用到一阶导，XGBOOST对函数进行二阶泰勒展开，同时可以自定义损失函数（只要函数一阶二阶可导）
- XGBOOST在代价函数中加入了正则项，用于控制模型的复杂度，正则项降低了模型的方差，降低过拟合
- 增加shrinkage，减小每棵树的影响力，为后面的树提供空间优化模型
- column subsampling，列特征抽样，防止过拟合，效果很好
- XGBOOST系数感知算法，可以自动处理有缺失值的样本
- <font color=blue>但是</font>每轮迭代都需要遍历很多次数据，同时需要预排序，保存数据和数据顺序，空间消耗大

> LightGBM，对XGBOOST优化

- 牺牲部分精确度（实际上牺牲的不多），但是提升效率
- 使用histogram算法替代之前的pre—sorted算法，减小内存消耗
- 同时使用的还有Gradient-based One-Side Sampling（GOSS）梯度单边采样（小梯度样本得到训练），Exclusive Feature Bundling 独立特征合并（稀疏矩阵变成稠密矩阵），Leaf-wise (Best-first) Tree Growth （提高精度，虽然会增加过拟合可能，但是前面的操作都有天然正则化效果），这里参考["这个同学的分享"](https://zhuanlan.zhihu.com/p/38516467)


